# -*- coding: utf-8 -*-
"""MINIST_SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GF5Hjt4oZ6wYlBfctm19qDwyFgLKd0OR

**PART 1 SVM CLASSIFICATION (PCA)**

**PART 2 SVM CLASSIFICATION (KPCA + LDA)**

**MNIST DATASET 70.000. 28 x 28 IMAGES OF HANDWRITTEN DIGITS FROM 0 TO 9.**

Importing Libraries
"""

import tensorflow as tf
import sklearn as sk
import numpy as np
import matplotlib.pyplot as plt
import sklearn.decomposition as skd
import sklearn.preprocessing as skp
import pandas as pd
import time

from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import cohen_kappa_score, mean_absolute_error, mean_squared_error

"""Loading Mnist Dataset"""

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("X_train data: ", X_train.shape)
print("X_test data: ", X_test.shape)

print("y_train data: ", y_train.shape)
print("y_test data: ", y_test.shape)

"""Concatenate Dataset in order to break the train - test sets in 60% - 40%."""

X_full = tf.concat([X_train, X_test], axis=0)
y_full = tf.concat([y_train, y_test], axis=0)

"""Select a subset of 10000 images from Mnist dataset and split it into 60%-40%"""

subset = 10000
X_subset = X_full[:subset].numpy() # convert datas to numpy arrays
y_subset = y_full[:subset].numpy()

# Split train - test sets to 60% - 40%
X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.4, random_state=0)

"""Split the test se into validation & test set (20% - 20%)"""

X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=0)

"""Counting the occurences of each class"""

from collections import Counter

class_counts = Counter(y_train)

plt.figure(figsize=(8, 6))
plt.bar(class_counts.keys(), class_counts.values())
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Class Distribution')
plt.xticks(range(10))
plt.show()

"""DATA PREPROCESSING

Normalization of images
"""

X_test = X_test / 255.0
X_train = X_train / 255.0
X_val = X_val / 255.0

"""Convert datas into flat vectors in order to train the svm."""

X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)
X_val = X_val.reshape(X_val.shape[0], -1)

"""Standardization of images, so all the features will have min=0 and std=1."""

scaler = skp.StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)

print("The shape of train data before dimentionality reduction: ", X_train.shape)
print("The shape of test data before dimentionality reduction: ", X_test.shape)
print("The shape of validation data before dimentionality reduction: ", X_val.shape)

"""Cumulative Explained Variance Diagram"""

combined=np.vstack((X_train, X_val)) # combine the X_train and X_val sets

pca = PCA() # pca without defining the number of components
pca.fit(combined) # pca for combined data

cumulative_variance = np.cumsum(pca.explained_variance_ratio_) # cumulative explained variance

plt.figure(figsize=(8, 6))
plt.plot(cumulative_variance)
plt.title('Cumulative Explained Variance vs. Number of Components')
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.grid(True)
plt.show()

"""Principal Component Analysis (PCA) for dimentionality reduction retaining  the 95% of the components."""

pca = skd.PCA(n_components=0.95)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)
X_val = pca.transform(X_val)

print("The shape of train data after dimentionality reduction: ", X_train.shape)
print("The shape of test data after dimentionality reduction: ", X_test.shape)
print("The shape of validation data after dimentionality reduction: ", X_val.shape)

"""**MODEL TRAINING**

Train SVM with different Kernels and default parameters in order to find the best one and improve it.
"""

kernels = ['linear', 'poly', 'rbf']

# store the results
results = {}

for kernel in kernels:

    # train svm
    svm = SVC(kernel=kernel)
    svm.fit(X_train, y_train)

    # prediction for validation set
    y_pred = svm.predict(X_val)

    # accuracy of validation set
    accuracy = accuracy_score(y_val, y_pred)

    results[kernel] = accuracy
    print("The kernel is {} and accuracy is {}".format(kernel, accuracy))

best_kernel = max(results, key=results.get)
print("The best kernel is {}".format(best_kernel))

"""Choose the best kernel and calculate the accuracy for the test set."""

svm = SVC(kernel=best_kernel)
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("The accuracy for test set is {} ".format(accuracy))

"""Train SVM with different Kernels and parameters

Linear Kernel with different c values (Regularization Parameter)
"""

# Function to train SVM with Linear Kernel
def linear_model_SVM (c):
    start_time = time.time()
    svm_classifier = SVC(kernel='linear', C=c)

    svm_classifier.fit(X_train, y_train)

    # Accuracy for train set
    pred = svm_classifier.predict(X_train)
    acc_train = accuracy_score(y_train, pred)
    train_set_accuracy.append(acc_train)
    print("Train Accuracy for C = {}: {}".format(c, acc_train))

    # Accuracy for validation set
    pred = svm_classifier.predict(X_val)
    acc_val = accuracy_score(y_val, pred)
    val_set_accuracy.append(acc_val)
    print("Validation Accuracy for C = {}: {}".format(c, acc_val))

    # Accuracy for test set
    pred = svm_classifier.predict(X_test)
    acc_test = accuracy_score(y_test, pred)
    test_set_accuracy.append(acc_test)
    print("Test Accuracy for C = {}: {}".format(c, acc_test))

    # metrics
    kappa = cohen_kappa_score(y_test, pred)
    mae = mean_absolute_error(y_test, pred)
    rmse = np.sqrt(mean_squared_error(y_test, pred))

    # Run time
    end_time = time.time()
    run_time = end_time - start_time

    results.append({
            'Kernel': 'linear',
            'C': c,
            'Train Accuracy': acc_train,
            'Validation Accuracy': acc_val,
            'Test Accuracy': acc_test,
            'Run time': run_time
            })

    metrics.append({
                'Kernel': 'linear',
                'C': c,
                'Test Accuracy': acc_test,
                'Kappa': kappa,
                'MAE': mae,
                'RMSE': rmse
                })

# Run SVM model for different C values
C_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100]


# Prepare variables to store the results
train_set_accuracy = []
val_set_accuracy = []
test_set_accuracy = []
results = []
metrics = []

for c in C_values:
  linear_model_SVM(c)

# calculate mean values of acc, kappa, mae, rmse
mean_test_accuracy = np.mean(test_set_accuracy)
mean_kappa = np.mean([metric['Kappa'] for metric in metrics])
mean_mae = np.mean([metric['MAE'] for metric in metrics])
mean_rmse = np.mean([metric['RMSE'] for metric in metrics])

# store the results
mean_results = {
    'Kernel': 'linear',
    'C': 'Mean',
    'Test Accuracy': mean_test_accuracy,
    'Mean Kappa': mean_kappa,
    'Mean MAE': mean_mae,
    'Mean RMSE': mean_rmse
}

# Print the results
plt.plot(C_values, train_set_accuracy,'.-',color='red')
plt.plot(C_values, val_set_accuracy,'.-',color='green')
plt.plot(C_values, test_set_accuracy,'.-',color='blue')
plt.legend(['Train Accuracy', 'Test Accuracy', 'Validation Accuracy'])
plt.title("Plot of accuracy vs c for training and test data")
plt.xlabel('c')
plt.ylabel('Accuracy')
plt.grid()

# Create a dataframe with the results
results_df = pd.DataFrame(results)
results_df

metrics_df = pd.DataFrame(metrics)
metrics_df

mean_results_linear_df = pd.DataFrame([mean_results])
mean_results_linear_df

"""Confusion Matrix"""

best_acc = np.argmax(test_set_accuracy)
best_C = C_values[best_acc]

svm_best = SVC(kernel='linear', C=best_C)
svm_best.fit(X_train, y_train)
y_pred = svm_best.predict(X_test)

from sklearn.metrics import confusion_matrix
import seaborn as sns

con_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(con_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

accuracy = np.trace(con_matrix) / np.sum(con_matrix)  # Trace is the sum of diagonal elements
print(f"Accuracy: {accuracy * 100:.2f}%")

"""Radial Basis Kernel with different C values and g (auto, scale)"""

# Function to train SVM with RBF Kernel
def rbf_model_SVM (c, g):

    start_time = time.time()

    svm_classifier = SVC(kernel='rbf', C=c, gamma=g)

    svm_classifier.fit(X_train, y_train)

    # Accuracy for train set
    pred = svm_classifier.predict(X_train)
    acc_train = accuracy_score(y_train, pred)
    train_set_accuracy.append(acc_train)
    print("Train Accuracy for C = {}: {}".format(c, acc_train))

    # Accuracy for validation set
    pred = svm_classifier.predict(X_val)
    acc_val = accuracy_score(y_val, pred)
    val_set_accuracy.append(acc_val)
    print("Validation Accuracy for C = {}: {}".format(c, acc_val))

    # Accuracy for test set
    pred = svm_classifier.predict(X_test)
    acc_test = accuracy_score(y_test, pred)
    test_set_accuracy.append(acc_test)
    print("Test Accuracy for C = {}: {}".format(c, acc_test))

    # metrics
    kappa = cohen_kappa_score(y_test, pred)
    mae = mean_absolute_error(y_test, pred)
    rmse = np.sqrt(mean_squared_error(y_test, pred))

    # Run time
    end_time = time.time()
    run_time = end_time - start_time

    results.append({
              'Kernel': 'RBF',
              'C': c,
              'Train Accuracy': acc_train,
              'Validation Accuracy': acc_val,
              'Test Accuracy': acc_test,
              'Run time: ': run_time
              })

    metrics.append({
                'Kernel': 'RBF',
                'C': c,
                'Test Accuracy': acc_test,
                'Kappa': kappa,
                'MAE': mae,
                'RMSE': rmse
                })

C_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100]

# Prepare variables to store the results
train_set_accuracy = []
val_set_accuracy = []
test_set_accuracy = []
results = []
metrics = []

for c in C_values:
    rbf_model_SVM(c, 'auto')

# calculate mean values of acc, kappa, mae, rmse
mean_test_accuracy = np.mean(test_set_accuracy)
mean_kappa = np.mean([metric['Kappa'] for metric in metrics])
mean_mae = np.mean([metric['MAE'] for metric in metrics])
mean_rmse = np.mean([metric['RMSE'] for metric in metrics])

# store the results
mean_results = {
    'Kernel': 'rbf',
    'C': 'Mean',
    'Test Accuracy': mean_test_accuracy,
    'Mean Kappa': mean_kappa,
    'Mean MAE': mean_mae,
    'Mean RMSE': mean_rmse
}

# Print the results
plt.plot(C_values, train_set_accuracy,'.-',color='red')
plt.plot(C_values, test_set_accuracy,'.-',color='blue')
plt.plot(C_values, val_set_accuracy,'.-',color='green')
plt.legend(['Train Accuracy', 'Test Accuracy', 'Validation Accuracy'])
plt.title("Plot of accuracy vs c for training - validation data - test data")
plt.xlabel('c')
plt.ylabel('Accuracy')
plt.grid()

# Create a dataframe with the results
results_df = pd.DataFrame(results)
results_df

metrics_df = pd.DataFrame(metrics)
metrics_df

mean_results_rbf_df = pd.DataFrame([mean_results])
mean_results_rbf_df

"""Confusion Matrix for RBF"""

best_acc = np.argmax(test_set_accuracy)
best_C = C_values[best_acc]

svm_best = SVC(kernel='linear', C=best_C)
svm_best.fit(X_train, y_train)
y_pred = svm_best.predict(X_test)

from sklearn.metrics import confusion_matrix
import seaborn as sns

con_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(con_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Polynomial Kernel with different C values"""

def poly_model_SVM (C, g):
      start_time = time.time()
      svm_classifier = SVC(kernel='poly', C=c, gamma=g)

      svm_classifier.fit(X_train, y_train)

      # Accuracy for train set
      pred = svm_classifier.predict(X_train)
      acc_train = accuracy_score(y_train, pred)
      train_set_accuracy.append(acc_train)
      print("Train Accuracy for C = {}: {}".format(c, acc_train))

      # Accuracy for validation set
      pred = svm_classifier.predict(X_val)
      acc_val = accuracy_score(y_val, pred)
      val_set_accuracy.append(acc_val)
      print("Validation Accuracy for C = {}: {}".format(c, acc_val))

      # Accuracy for test set
      pred = svm_classifier.predict(X_test)
      acc_test = accuracy_score(y_test, pred)
      test_set_accuracy.append(acc_test)
      print("Test Accuracy for C = {}: {}".format(c, acc_test))

      # metrics
      kappa = cohen_kappa_score(y_test, pred)
      mae = mean_absolute_error(y_test, pred)
      rmse = np.sqrt(mean_squared_error(y_test, pred))

      # Run time
      end_time = time.time()
      run_time = end_time - start_time

      results.append({
              'Kernel': 'Polynomial',
              'C': c,
              'Train Accuracy': acc_train,
              'Validation Accuracy': acc_val,
              'Test Accuracy': acc_test
                  })

      metrics.append({
                'Kernel': 'Polynomial',
                'C': c,
                'Test Accuracy': acc_test,
                'Kappa': kappa,
                'MAE': mae,
                'RMSE': rmse
                })

C_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100]

train_set_accuracy = []
val_set_accuracy = []
test_set_accuracy = []
results = []
metrics = []

for c in C_values:
  poly_model_SVM(c, g='scale')


# calculate mean values of acc, kappa, mae, rmse
mean_test_accuracy = np.mean(test_set_accuracy)
mean_kappa = np.mean([metric['Kappa'] for metric in metrics])
mean_mae = np.mean([metric['MAE'] for metric in metrics])
mean_rmse = np.mean([metric['RMSE'] for metric in metrics])

# store the results
mean_results = {
    'Kernel': 'poly',
    'C': 'Mean',
    'Test Accuracy': mean_test_accuracy,
    'Mean Kappa': mean_kappa,
    'Mean MAE': mean_mae,
    'Mean RMSE': mean_rmse
}

# Print the results
plt.plot(C_values, train_set_accuracy,'.-',color='red')
plt.plot(C_values, test_set_accuracy,'.-',color='blue')
plt.plot(C_values, val_set_accuracy,'.-',color='green')
plt.legend(['Train Accuracy', 'Test Accuracy', 'Validation Accuracy'])
plt.title("Plot of accuracy vs c for training - validation data - test data")
plt.xlabel('c')
plt.ylabel('Accuracy')
plt.grid()

# Create a dataframe with the results
results_df = pd.DataFrame(results)
results_df

metrics_df = pd.DataFrame(metrics)
metrics_df

mean_results_poly_df = pd.DataFrame([mean_results])
mean_results_poly_df

"""Confusion Matrix for Polynomial"""

best_acc = np.argmax(test_set_accuracy)
best_C = C_values[best_acc]

svm_best = SVC(kernel='linear', C=best_C)
svm_best.fit(X_train, y_train)
y_pred = svm_best.predict(X_test)

from sklearn.metrics import confusion_matrix
import seaborn as sns

con_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(con_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Mean Metrics Comnined all"""

mean_results_all = pd.concat([mean_results_linear_df, mean_results_rbf_df, mean_results_poly_df])
mean_results_all

"""Cross Validation Method - Grid Search to find the best hyperparameters"""

# defining Kernels
kernels = ['linear', 'poly', 'rbf']

# results
results = []

# defining parameter range
param_grid1 = {'C': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.05, 0.1, 1, 10, 100],

}

param_grid2 = {'C': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.05, 0.1, 1, 10, 100],
              'gamma': ['scale', 'auto']
}


# svm classifier
for kernel in kernels:
  if kernel == 'linear':
    classifier = SVC(kernel=kernel)
    grid_search = GridSearchCV(classifier, param_grid1, cv=5, n_jobs=-1, verbose = 0)
  else:
    classifier = SVC(kernel=kernel)
    grid_search = GridSearchCV(classifier, param_grid2, cv=5, n_jobs=-1, verbose = 0)


  # fitting the model for grid search
  grid_search.fit(X_train, y_train)

  # find the best parameter for grid search
  best_parameters = grid_search.best_params_
  accuracy = grid_search.best_score_

  results.append({
              'Kernel': kernel,
              'Best Parameters': best_parameters,
              'Accuracy': accuracy
                  })

# Create a dataframe with the results
results_df = pd.DataFrame(results)
results_df

# find the best kernel
best_kernel = results_df.loc[results_df['Accuracy'].idxmax()]['Kernel']
print("The best kernel is {}".format(best_kernel))

# find the best parameters from the GridSearchCV
if best_kernel == 'linear':
  best_parameters = results_df.loc[results_df['Kernel']=='linear'].iloc[0]['Best Parameters']
  clf = SVC(kernel=best_kernel, C=best_parameters['C'])

else:
  best_parameters = results_df.loc[results_df['Kernel']== best_kernel].iloc[0]['Best Parameters']
  clf = SVC(kernel=best_kernel, C=best_parameters['C'], gamma=best_parameters['gamma'])

# validation set accuracy
y_pred = clf.predict(X_val)
accuracy_svm = accuracy_score(y_val, y_pred)
print("The accuracy for validation set is {} ".format(accuracy_svm))

# test set accuracy
clf.fit(X_train, y_train)
y_pred_test = clf.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred_test)
print("The accuracy for test set is {} ".format(accuracy_svm))

from sklearn.metrics import confusion_matrix, classification_report

con_matrix = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(10, 8))
sns.heatmap(con_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print("Classification Report:\n", classification_report(y_test, y_pred_test))

"""Show Correct and Incorrect Classification Results"""

svm_results = y_pred[:20]
yTest_results = y_test[:20]

# DataFrame with predicted and actual values
results_df = pd.DataFrame({
    'Prediction': svm_results,
    'Actual Value': yTest_results
})

# print the dataframe
print(results_df)

# Create lists for correct and incorrect predictions
correct_predictions = []
incorrect_predictions = []

for i in range(len(svm_results)):
    if svm_results[i] == yTest_results[i]:
        correct_predictions.append(i)
    else:
        incorrect_predictions.append(i)

print("Correct Predictions:", correct_predictions)
print("Incorrect Predictions:", incorrect_predictions)

"""KPCA + LDA

Loading Mnist Dataset
"""

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

"""Concatenate Dataset in order to break the train - test sets in 60% - 40%."""

X_full = tf.concat([X_train, X_test], axis=0)
y_full = tf.concat([y_train, y_test], axis=0)

"""Choose a subset of 10000 images from Mnist dataset and split it into 60%-40%"""

subset = 10000
X_subset = X_full[:subset].numpy() # convert datas to numpy arrays
y_subset = y_full[:subset].numpy()

# Split train - test sets to 60% - 40%
X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.4, random_state=0)

"""Split the test se into validation & test set (20% - 20%)"""

X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=0)

"""DATA PREPROCESSING

Normalization of Images
"""

X_test = X_test / 255.0
X_train = X_train / 255.0
X_val = X_val / 255.0

"""Convert datas into flat vectors in order to train the svm."""

X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)
X_val = X_val.reshape(X_val.shape[0], -1)

"""Standardization of images, so all the features will have min=0 and std=1."""

scaler = skp.StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)

print("The shape of train data before dimentionality reduction: ", X_train.shape)
print("The shape of test data before dimentionality reduction: ", X_test.shape)
print("The shape of validation data before dimentionality reduction: ", X_val.shape)

"""Model Training (KPCA + LDA)

Linear Kernel
"""

from sklearn import decomposition
from sklearn.decomposition import KernelPCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.metrics import cohen_kappa_score, mean_absolute_error, mean_squared_error

# define kernels
kernels = ['linear', 'poly', 'rbf']

# define C parameter values
C_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1]

# save results
results = []

# save metrics
metrics = []

# kpca + lda for 3 different kernels
for kernel in kernels:

    # hold accuracy values for every c value in a list
    train_set_accuracy = []
    test_set_accuracy = []
    val_set_accuracy = []

    # kpca
    kpca = KernelPCA(kernel=kernel)
    X_train_kpca = kpca.fit(X_train)

    # cumulative explained variance
    eigenvalues = kpca.eigenvalues_
    explained_variance_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)

    # number of compnents that explain the 95% of variance
    n_components_95 = np.argmax(explained_variance_ratio >= 0.95) +1

    # apply kpca with selected number of components
    kpca = KernelPCA(kernel=kernel, n_components=n_components_95)
    X_train_kpca = kpca.fit_transform(X_train)
    X_val_kpca = kpca.transform(X_val)
    X_test_kpca = kpca.transform(X_test)

    # lda
    lda = LDA(n_components = 9)
    X_train_lda = lda.fit_transform(X_train_kpca, y_train)
    X_val_lda = lda.transform(X_val_kpca)
    X_test_lda = lda.transform(X_test_kpca)

    for c in C_values:

        # svm training for linear Kernel
        svm = SVC(kernel='linear', C=c)
        svm.fit(X_train_lda, y_train)

        # prediction for every kernel pca
        y_pred_train = svm.predict(X_train_lda)
        y_pred_val = svm.predict(X_val_lda)
        y_test_pred = svm.predict(X_test_lda)

        # train set accuracy
        train_accuracy = accuracy_score(y_train, y_pred_train)
        train_set_accuracy.append(train_accuracy)

        # validation set accuracy
        val_accuracy = accuracy_score(y_val, y_pred_val)
        val_set_accuracy.append(val_accuracy)

        # test set accuracy
        test_accuracy = accuracy_score(y_test, y_test_pred)
        test_set_accuracy.append(test_accuracy)

        # calculate metrics
        kappa = cohen_kappa_score(y_test, y_test_pred)
        mae = mean_absolute_error(y_test, y_test_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))


        results.append({
                'Kernel (KPCA)': kernel,
                'n_components': n_components_95,
                'C': c,
                'Train Accuracy': train_accuracy,
                'Validation Accuracy': val_accuracy,
                'Test Accuracy': test_accuracy
                })

        metrics.append({
                'C': c,
                'Test Accuracy': test_accuracy,
                'Kernel (KPCA)': kernel,
                'Kappa': kappa,
                'MAE': mae,
                'RMSE': rmse
                })


    # Print the results
    plt.figure(figsize=(8, 6))
    plt.plot(C_values, train_set_accuracy, '.-', color='red', label=f'Train ({kernel} kernel)')
    plt.plot(C_values, val_set_accuracy, '.-', color='green', label=f'Validation ({kernel} kernel)')
    plt.plot(C_values, test_set_accuracy, '.-', color='blue', label=f'Test ({kernel} kernel)')
    plt.title(f"Accuracy vs C for {kernel} KernelPCA + LDA", fontsize=14)
    plt.xlabel('C (Regularization Parameter)', fontsize=12)
    plt.ylabel('Accuracy (%)', fontsize=12)
    plt.xscale('log')  # Log scale for C values
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)
    plt.legend()
    plt.tight_layout()
    plt.show()

from IPython.display import display

# DataFrame Results
results_df = pd.DataFrame(results)
metrics_df = pd.DataFrame(metrics)

# create 3 different arrays for Kernel PCA
results_linear = results_df[results_df['Kernel (KPCA)'] == 'linear']
results_poly = results_df[results_df['Kernel (KPCA)'] == 'poly']
results_rbf = results_df[results_df['Kernel (KPCA)'] == 'rbf']

# create 3 different arrays for metrics
metrics_linear = metrics_df[metrics_df['Kernel (KPCA)'] == 'linear']
metrics_poly = metrics_df[metrics_df['Kernel (KPCA)'] == 'poly']
metrics_rbf = metrics_df[metrics_df['Kernel (KPCA)'] == 'rbf']

# display results & metrics
display(results_linear)
display(results_poly)
display(results_rbf)

display(metrics_linear)
display(metrics_poly)
display(metrics_rbf)

"""Kernel PCA + LDA GRID SEARCH - CROSS VALIDATION

Linear KPCA - Cross 5 Fold - Validation Grid Search
"""

from sklearn.model_selection import GridSearchCV
from sklearn import decomposition
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.decomposition import KernelPCA
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
import seaborn as sns
from sklearn.model_selection import learning_curve
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

# Results Storage
results_linear = []

# Calculate n_components_95 for KernelPCA
kpca_temp = KernelPCA(kernel='linear')
kpca_temp.fit(X_train)

eigenvalues = kpca_temp.eigenvalues_
explained_variance_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)
n_components_95 = np.argmax(explained_variance_ratio >= 0.95) +1

# Create a pipeline
kpca = KernelPCA(kernel='linear', n_components=n_components_95)
lda = LDA(n_components = 9)
svc = SVC(kernel='linear')

pipeline = make_pipeline(kpca, lda, svc)

# Define Grid Search Hyperparameters for Linear SVC
param_grid = {
    'svc__C':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.05, 0.1, 1]
}

# Run Grid Search 5-Fold Cross Validation
grid_search_linear = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search_linear.fit(X_train, y_train)


# Find Best Model & Results
best_parameters_linear = grid_search_linear.best_params_
best_model_linear = grid_search_linear.best_estimator_
accuracy_1 = grid_search_linear.best_score_

results_linear.append({
    'Best Parameters': best_parameters_linear,
    'Accuracy': accuracy_1,
    'Number of Components': n_components_95,
    })

# Create a dataframe to display the results
results_linear_df = pd.DataFrame(results_linear)
results_linear_df

# Test the model to the validation set
y_pred_val = best_model_linear.predict(X_val)
accuracy_val = accuracy_score(y_val, y_pred_val)
print(f"Accuracy on the validation set: {accuracy_val:.4f}")

# Test the model to the test set
y_pred_test = best_model_linear.predict(X_test)
accuracy_test = accuracy_score(y_test, y_pred_test)
print(f"Accuracy on the test set: {accuracy_test:.4f}")

# Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred_test))

# Confusion Matrix (Heatmap)
confusion_matrix_1 = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix_1, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix Linear Kernel PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""RBF KPCA - Cross 5 Fold - Validation Grid Search"""

from sklearn.model_selection import GridSearchCV
from sklearn import decomposition
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.decomposition import KernelPCA
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score

results_rbf = []

# Calculate n_components_95 for KernelPCA
kpca_temp = KernelPCA(kernel='rbf')
kpca_temp.fit(X_train)

eigenvalues = kpca_temp.eigenvalues_
explained_variance_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)
n_components_95 = np.argmax(explained_variance_ratio >= 0.95) +1

kpca = KernelPCA(kernel='rbf', n_components=n_components_95, gamma=None)

lda = LDA(n_components = 9)

svc = SVC(kernel='rbf')

pipeline = make_pipeline(kpca, lda, svc)

param_grid = {
    'svc__C':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.05, 0.1, 1]
}


grid_search_rbf = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search_rbf.fit(X_train, y_train)


# Find best grid search parameters
best_parameters_rbf = grid_search_rbf.best_params_
best_model_rbf = grid_search_rbf.best_estimator_
accuracy_2 = grid_search_rbf.best_score_

results_rbf.append({
    'Best Parameters': best_parameters_rbf,
    'Accuracy': accuracy_2,
    'Number of Components': n_components_95,
    })

results_rbf_df = pd.DataFrame(results_rbf)
results_rbf_df

y_pred_val = best_model_rbf.predict(X_val)
accuracy_val = accuracy_score(y_val, y_pred_val)
print(f"Accuracy on the validation set: {accuracy_val:.4f}")

y_pred_test = best_model_rbf.predict(X_test)
accuracy_test = accuracy_score(y_test, y_pred_test)
print(f"Accuracy on the test set: {accuracy_test:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
print("Classification Report:")
print(classification_report(y_test, y_pred_test))

# Confusion Matrix (Heatmap)
confusion_matrix_2 = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix_2, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix RBF Kernel PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Poly KPCA - Cross 5 Fold - Validation Grid Search"""

from sklearn.model_selection import GridSearchCV
from sklearn import decomposition
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.decomposition import KernelPCA
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
import seaborn as sns
from sklearn.model_selection import learning_curve
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

# Results Storage
results_poly = []

# Calculate n_components_95 for KernelPCA
kpca_temp = KernelPCA(kernel='poly')
kpca_temp.fit(X_train)

eigenvalues = kpca_temp.eigenvalues_
explained_variance_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)
n_components_95 = np.argmax(explained_variance_ratio >= 0.95) +1

# Create a pipeline
kpca = KernelPCA(kernel='poly', n_components=n_components_95, gamma=None)
lda = LDA(n_components = 9)
svc = SVC(kernel='poly')

pipeline = make_pipeline(kpca, lda, svc)

# Define Grid Search Hyperparameters for Linear SVC
param_grid = {
    'svc__C':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.05, 0.1, 1]
}

# Run Grid Search 5-Fold Cross Validation
grid_search_poly = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search_poly.fit(X_train, y_train)


# Find Best Model & Results
best_parameters_poly = grid_search_poly.best_params_
best_model_poly = grid_search_poly.best_estimator_
accuracy_3 = grid_search_poly.best_score_

results_poly.append({
    'Best Parameters': best_parameters_poly,
    'Accuracy': accuracy_3,
    'Number of Components': n_components_95,
    })

results_poly_df = pd.DataFrame(results_poly)
results_poly_df

y_pred_val = best_model_poly.predict(X_val)
accuracy_val = accuracy_score(y_val, y_pred_val)
print(f"Accuracy on the validation set: {accuracy_val:.4f}")

y_pred_test = best_model_poly.predict(X_test)
accuracy_test = accuracy_score(y_test, y_pred_test)
print(f"Accuracy on the test set: {accuracy_test:.4f}")

print("Classification Report:")
print(classification_report(y_test, y_pred_test))

# Confusion Matrix (Heatmap)
confusion_matrix_3 = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix_3, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix Poly Kernel PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""BEST KPCA RBF (c=0.0001, gamma=None)"""

from sklearn.model_selection import GridSearchCV
from sklearn import decomposition
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.decomposition import KernelPCA
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
import seaborn as sns
from sklearn.model_selection import learning_curve
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

kpca_best = KernelPCA(kernel='rbf', n_components=2748)
X_train_kpca_best = kpca_best.fit_transform(X_train)
X_val_kpca_best = kpca_best.transform(X_val)
X_test_kpca_best = kpca_best.transform(X_test)

lda_best = LDA(n_components=9)
X_train_lda_best = lda_best.fit_transform(X_train_kpca_best, y_train)
X_val_lda_best = lda_best.transform(X_val_kpca_best)
X_test_lda_best = lda_best.transform(X_test_kpca_best)

svm_best = SVC(kernel='linear', C=0.0001, probability=True)
svm_best.fit(X_train_lda_best, y_train)

y_pred_prob = svm_best.predict_proba(X_test_lda_best)

pred_y = svm_best.predict(X_test_lda_best)
accuracy = accuracy_score(y_test, pred_y)
print(f"Accuracy: {accuracy}")

"""Learning Curves"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

# accuracy
train_sizes = np.linspace(0.1, 1.0, 10) # create learning curves
train_sizes, train_scores, val_scores = learning_curve(svm_best, X_train_lda_best, y_train, cv=5, scoring='accuracy', train_sizes=train_sizes)

# calculate means and std
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# plot learning curves
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training Accuracy', color='blue')
plt.plot(train_sizes, val_mean, label='Validation Accuracy', color='red')

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.1)
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color='red', alpha=0.1)

plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.title('Learning Curves')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

train_sizes = np.linspace(0.1, 1.0, 10) # create learning curves for the pipeline
train_size_loss, train_Scores_loss, val_Scores_loss = learning_curve(svm_best, X_train_lda_best, y_train, cv=5, scoring='neg_log_loss', train_sizes=train_sizes)

# Calculate means and std
train_mean_loss = -np.mean(train_Scores_loss, axis=1)
train_std_loss = np.std(train_Scores_loss, axis=1)
val_mean_loss = -np.mean(val_Scores_loss, axis=1)
val_std_loss = np.std(val_Scores_loss, axis=1)

# plot learning curves
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean_loss, label='Training Loss', color='blue')
plt.plot(train_sizes, val_mean_loss, label='Validation Loss', color='red')

plt.fill_between(train_sizes, train_mean_loss - train_std_loss, train_mean_loss + train_std_loss, color='blue', alpha=0.1)

plt.xlabel('Training Set Size')
plt.ylabel('Loss')
plt.title('Learning Curves')
plt.legend()
plt.grid(True)
plt.show()