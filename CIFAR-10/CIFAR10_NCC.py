# -*- coding: utf-8 -*-
"""CIRA10_NCC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLpegQfBfIQU9uPFfNWtmNhxZ0rTtfKI
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sns
import sklearn.preprocessing as skp


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import Normalizer # For Normalization
from sklearn.preprocessing import StandardScaler # For Standardization
from sklearn.neighbors import NearestCentroid
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestCentroid

cifar10 = tf.keras.datasets.cifar10
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

print("X_train data: ", X_train.shape)
print("X_test data: ", X_test.shape)

print("y_train data: ", y_train.shape)
print("y_test data: ", y_test.shape)

X_full = tf.concat([X_train, X_test], axis=0)
y_full = tf.concat([y_train, y_test], axis=0)

subset = 10000
X_subset = X_full[:subset].numpy() # convert datas to numpy arrays
y_subset = y_full[:subset].numpy()

# Split train - test sets to 60% - 40%
X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.4, random_state=0)

X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=0)

X_test = X_test / 255.0
X_train = X_train / 255.0
X_val = X_val / 255.0

X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)
X_val = X_val.reshape(X_val.shape[0], -1)

scaler = skp.StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)

# pca for dimensionality reduction
pca_cifar = PCA(n_components=0.95)
X_train = pca_cifar.fit_transform(X_train)
X_val = pca_cifar.transform(X_val)
X_test = pca_cifar.transform(X_test)

print("X_train.shape: {}\ny_train.shape: {}\nX_test.shape: {}\ny_test.shape: {}\nX_val.shape: {}\ny_val.shape: {}".format(X_train.shape, y_train.shape,
                                                                                                                          X_test.shape, y_test.shape,
                                                                                                                          X_val.shape, y_val.shape))

ncc_model = NearestCentroid()

param_grid = {
    'metric': ['euclidean', 'manhattan']
}

grid_search = GridSearchCV(ncc_model, param_grid, cv=5, n_jobs=-1)  # cv=5 για 5-fold cross-validation

grid_search.fit(X_train, y_train.ravel())

best_model_ncc = grid_search.best_estimator_
print("Best Model Parameters:", grid_search.best_params_)
print("Best Model Accuracy (CV):", grid_search.best_score_)

# Validation set Accuracy
y_val_pred = best_model_ncc.predict(X_val)
val_accuracy = accuracy_score(y_val.ravel(), y_val_pred)
print("Validation Set Accuracy:", val_accuracy)

# Test set Accuracy
y_test_pred = best_model_ncc.predict(X_test)
test_accuracy = accuracy_score(y_test.ravel(), y_test_pred)
print("Test Set Accuracy:", test_accuracy)

"""Confusion Matrix"""

from sklearn.metrics import confusion_matrix, classification_report

# Confusion Matrix (Heatmap)
confusion_matrix = confusion_matrix(y_test.ravel(), y_test_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix NCC + PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Classification Report"""

from sklearn.metrics import classification_report, confusion_matrix
print("Classification Report:")
print(classification_report(y_test.ravel(), y_test_pred))

ncc_pred = best_model_ncc.predict(X_test)
ncc_accuracy = accuracy_score(y_test.ravel(), ncc_pred)
print("NCC Test set Accuracy:", ncc_accuracy)

ncc_results = ncc_pred[:20].ravel()
yTest_results = y_test[:20].ravel()

# DataFrame with predicted and actual values
results_df = pd.DataFrame({
    'Prediction': ncc_results,
    'Actual Value': yTest_results
})

# print the dataframe
print(results_df)

# Create lists for correct and incorrect predictions
correct_predictions = []
incorrect_predictions = []

for i in range(len(ncc_results)):
    if ncc_results[i] == yTest_results[i]:
        correct_predictions.append(i)
    else:
        incorrect_predictions.append(i)

print("Correct Predictions:", correct_predictions)
print("Incorrect Predictions:", incorrect_predictions)

"""KPCA + LDA - 5 & 10 FOLD CROSS VALIDATION"""

from sklearn.decomposition import KernelPCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

ncc = NearestCentroid()

kernels = ['linear', 'poly', 'rbf']


results = {}

for kernel in kernels:

    # kpca
    kpca = KernelPCA(kernel=kernel)
    X_train_kpca = kpca.fit_transform(X_train)

    # cumulative explained variance
    eigenvalues = kpca.eigenvalues_
    explained_variance_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)

    # number of compnents that explain the 95% of variance
    n_components_95 = np.argmax(explained_variance_ratio >= 0.95) +1

    kpca = KernelPCA(n_components_95, kernel=kernel)
    X_train_kpca = kpca.fit_transform(X_train)
    X_test_kpca = kpca.transform(X_test)

    lda = LDA(n_components=9)
    X_train_lda = lda.fit_transform(X_train_kpca, y_train.ravel())
    X_test_lda = lda.transform(X_test_kpca)


    param_grid = {
    'metric': ['euclidean', 'manhattan']
    }

    grid_search = GridSearchCV(ncc, param_grid, cv=10, n_jobs=-1)
    grid_search.fit(X_train_lda, y_train.ravel())


    results[kernel] = {
        'Best n_components': n_components_95,
        'Best Parameters for NCC': grid_search.best_params_,
        'Best Score for NCC': grid_search.best_score_
       }

results_df = pd.DataFrame(results)
results_df

best_model_ncc = grid_search.best_estimator_

y_pred = best_model_ncc.predict(X_test_lda)

accuracy = accuracy_score(y_test.ravel(), y_pred)

print("Accuracy:", accuracy)

"""NCC RBF KERNEL PCA + LDA"""

from sklearn.decomposition import KernelPCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# kpca
kpca = KernelPCA(kernel='rbf')
X_train_kpca = kpca.fit(X_train)

# cumulative explained variance
eigenvalues = kpca.eigenvalues_
explained_variance_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)

# number of compnents that explain the 95% of variance
n_components_95 = np.argmax(explained_variance_ratio >= 0.95) +1

# kpca
kpca = KernelPCA(n_components_95, kernel='rbf')
X_train_kpca = kpca.fit_transform(X_train)
X_val_kpca = kpca.transform(X_val)
X_test_kpca = kpca.transform(X_test)

# lda
lda = LDA(n_components=9)
X_train_lda = lda.fit_transform(X_train_kpca, y_train.ravel())
X_val_lda = lda.transform(X_val_kpca)
X_test_lda = lda.transform(X_test_kpca)

# ncc
ncc = NearestCentroid(metric='euclidean')
ncc.fit(X_train_lda, y_train.ravel())

# predict
y_pred = ncc.predict(X_test_lda)

# accuracy
accuracy = accuracy_score(y_test.ravel(), y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import confusion_matrix, classification_report

class_report = classification_report(y_test.ravel(), y_pred)
print("Classification Report:")
print(class_report)

# Confusion Matrix (Heatmap)
cm = confusion_matrix(y_test.ravel(), y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix RBF Kernel PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""NCC POLYNOMIAL KPCA + LDA"""

from sklearn.decomposition import KernelPCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# kpca
kpca = KernelPCA(kernel='poly')
X_train_kpca = kpca.fit(X_train)

# cumulative explained variance
eigenvalues = kpca.eigenvalues_
explained_variance_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)

# number of compnents that explain the 95% of variance
n_components_95 = np.argmax(explained_variance_ratio >= 0.95) +1

kpca = KernelPCA(n_components_95, kernel='poly', degree= 6, coef0=1)
X_train_kpca = kpca.fit_transform(X_train)
X_test_kpca = kpca.transform(X_test)

lda = LDA(n_components=9)
X_train_lda = lda.fit_transform(X_train_kpca, y_train.ravel())
X_test_lda = lda.transform(X_test_kpca)

ncc = NearestCentroid(metric='euclidean')
ncc.fit(X_train_lda, y_train.ravel())

y_pred = ncc.predict(X_test_lda)
accuracy = accuracy_score(y_test.ravel(), y_pred)
print("Accuracy:", accuracy)